# app/services/chat_logic.py

import re
import requests
from datetime import datetime
from openai import OpenAI

# æ¨¡å¼åˆ‡æ¢ï¼šTrue ä½¿ç”¨æœ¬åœ° Ollamaï¼ŒFalse ä½¿ç”¨ DeepSeek API
USE_OLLAMA = False

# æœ¬åœ°æ¨¡å‹é…ç½®
OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "deepseek-r1:1.5b"  # æœ¬åœ°æ¨¡å‹åç§°

# DeepSeek API å®¢æˆ·ç«¯é…ç½®
client = OpenAI(
    api_key="sk-e1927ee1ea204a22b49fa3667f70a033",
    base_url="https://api.deepseek.com"
)

MAX_HISTORY = 8  # ä¿ç•™æœ€è¿‘8è½®å¯¹è¯

def process_chat(full_history, instruction=None):
    try:
        optimized_history = optimize_context(full_history.copy())

        # å¦‚æœæä¾›äº†é¢å¤–çš„ç³»ç»ŸæŒ‡ä»¤ï¼Œæ’å…¥åˆ°å†å²è®°å½•çš„æœ€å‰é¢
        if instruction:
            optimized_history.insert(0, {"role": "system", "content": instruction})

        if USE_OLLAMA:
            return chat_with_ollama(optimized_history)
        else:
            return chat_with_api(optimized_history)

    except Exception as e:
        log_error(e)
        return "å½“å‰æœåŠ¡ç¹å¿™ï¼Œè¯·ç¨åå†è¯•"

def chat_with_api(messages):
    """è°ƒç”¨ DeepSeek å®˜æ–¹ API èŠå¤©"""
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
        temperature=0.7,
        max_tokens=1000
    )
    return response.choices[0].message.content

def chat_with_ollama(messages):
    # æ‹¼æ¥ prompt
    prompt = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        if role == "system":
            prompt += f"System: {content}\n"
        elif role == "user":
            prompt += f"User: {content}\n"
        elif role == "assistant":
            prompt += f"Assistant: {content}\n"
    prompt += "Assistant:"

    try:
        print("ğŸ”§ æ­£åœ¨å‘ Ollama å‘é€è¯·æ±‚...")
        print("ğŸ“¤ Prompt:")
        print(prompt)

        res = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )

        print(f"ğŸ“¥ çŠ¶æ€ç : {res.status_code}")
        print(f"ğŸ“¥ å“åº”: {res.text}")

        if res.status_code == 200:
            response = res.json().get("response", "").strip()
            if response:
                # åœ¨ç»ˆç«¯æ‰“å°å®Œæ•´å“åº”
                print("å®Œæ•´å“åº”:")
                print(response)

                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹
                cleaned_response = re.sub(r"<think>.*?</think>", "", response, flags=re.DOTALL).strip()
                return cleaned_response
            else:
                log_error("Ollama returned empty response.")
                return "æœªèƒ½è·å¾—æœ‰æ•ˆçš„æ¨¡å‹å›å¤"
        else:
            log_error(f"Ollama error: {res.status_code} - {res.text}")
            return "æœ¬åœ°æ¨¡å‹æœåŠ¡æš‚ä¸å¯ç”¨"

    except requests.exceptions.RequestException as e:
        log_error(f"Ollama request error: {e}")
        return "æœ¬åœ°æ¨¡å‹æœåŠ¡æš‚ä¸å¯ç”¨"

def optimize_context(history):
    """ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦ç­–ç•¥"""
    if len(history) > MAX_HISTORY + 1:
        return [history[0]] + history[-MAX_HISTORY:]
    return history

def log_error(error):
    """é”™è¯¯æ—¥å¿—è®°å½•"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open("chat_errors.log", "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] {str(error)}\n")
    except Exception as file_error:
        print(f"æ—¥å¿—å†™å…¥å¤±è´¥ï¼š{file_error}")
